* <<<PE503>>> NATURAL LANGUAGE PROCESSING
:properties:
:author: Dr. D. THenmozhi and Mr. B. Senthil Kumar
:date: 09-03-2021
:end:

#+begin_comment
- 1. Combined Unit 2 and 3 of AU into Unit 2, Unit 4 and 5 of AU into Unit 3 to give emphasis on
   NLP applications
- 2. For changes, see the indidual units
- 3. The unit headings are similar to M.E syllabus with addition and deletion of topics except Unit 4. 
     Unit 4 and 5 are focussing on NLP applications. Removed NLP using Python
- 4. Five Course outcomes specified and aligned with units
- 5. Not Applicable
#+end_comment

#+startup: showall

{{{credits}}}
|L|T|P|C|
|3|0|0|3|

** COURSE OBJECTIVES
- To learn language models
- To learn text pre processing techniques
- To understand the levels of knowledge in language processing
- To develop NLP applications.
- To apply traditional learning and deep learning for NLP applications.

{{{unit}}}
| UNIT I | TEXT PRE-PROCESSING AND LANGUAGE MODELLING | 9 |
Knowledge in language processing -- NLP applications; -- Regular Expressions -- Words -- 
Corpora -- Text Normalization -- Minimum Edit distance -- N-gram language models -- 
Neural language models - RNNs as language models


#+begin_comment

- 1. Removed grammar based language models
- 2. Added Neural language models
- 3. Moved text pre processing from Unit II to Unit 1

#+end_comment

{{{unit}}}
| UNIT II | WORD LEVEL AND SYNTACTIC ANALYSIS | 9 |
Word Level Analysis: Word classes -- Part-of-Speech Tagging: HMM POS tagging; Named Entities (NE): NE Tagging -- 
Conditional Random Field NE recognizer; Syntactic Analysis: Constituency -- Context-free grammar 
-- Grammar rules -- Treebanks; Parsing: Top-down -- Bottom-up -- Ambiguity -- CKY Parsing -- 
Shallow parsing -- Dependency parsing 


#+begin_comment

- 1. Removed Early algorithm
- 2. Added Shallow parsing
- 3. Moved pre processing to Unit I from Unit II
- 4. Added NE tagging in word level analysis
#+end_comment


{{{unit}}}
| UNIT III | SEMANTIC ANALYSIS | 9 |
Vector Semantics -- Words and Vectors -- Cosine similarity -- Tf-idf -- Positive PMI -- Word2vec-- 
Semantic properties of embeddings; Lexical Semantics: Word Senses -- Relations between senses -- 
WordNet -- Word Sense Disambiguation


#+begin_comment

- 1. Removed basic representations of semantics
- 2. Added Vector semantics
- 3. Removed thematic roles from lexical semantics
- 4. Added Word embeddings

#+end_comment

{{{unit}}}

| UNIT IV | COREFERENCE RESOLUTION AND MACHINE TRANSLATION  | 9 |
Coreference Resolution: Coreference phenomena -- Mention detection -- Mention-pair architecture;
RNNs for sequence labeling and classification --  Stacked and Bi-directional RNN -- Machine Translation(MT): 
Lexical divergence and typology -- Encoder-Decoder with RNNs --  MT Evaluation; 



#+begin_comment

- 1. Added Mention detection
- 2. Removed Centering and other basic algorithms for reference resolution
- 3. Added deep learning for sequence labeling and classification
- 4. Moved machine translation from Unit V to Unit IV
#+end_comment

{{{unit}}}
| UNIT V | NLP Applications | 9 |
Sentiment Classification: Naive Bayes classifier -- Optimizing for Sentiment Analysis -- Evaluation; 
Information Extraction: Relation extraction; Information Retrieval ; IR-based Factoid Question Answering: 
IR-based QA Datasets -- Answer span extraction; 



#+begin_comment

- 1. Moved IR and IE from Unit IV to Unit V
- 2. Added Sentiment analysis

#+end_comment


Group Projects:
1) CRF POS/NER Tagging
2) Word Generation using N-grams
3) CFG / Dependency parsing
4) Semantics of Word2vec embeddings
5) Neural machine translation - NMT
6) Sequence classification / Naive Bayes classifier for Sentiment analysis
7) Relation Extraction



\hfill *Total Periods: 45*

** COURSE OUTCOMES
Upon the completion of the course the students should be able to: 
- Apply text pre processing techniques and build the language models (K3)
- Explain the basic levels of knowledge namely word level and syntax level in language processing (K2)
- Apply computational methods in lexical and vector semantics (K3)
- Apply NLP techniques for discourse processing and build machine translation systems using deep learning (K3)
- Apply learning methodologies for different NLP applications (K3).
- Develop any NLP application by using NLP techniques and learning methodologies (K4)

** TEXT BOOKS
1. Daniel Jurafsky and James H Martin, ``Speech and Language
   Processing: An introduction to Natural Language Processing,
   Computational Linguistics and Speech Recognition'', 2nd Edition,
   Prentice Hall, 2008.
2. https://web.stanford.edu/~jurafsky/slp3/


** REFERENCES
1. Christopher D Manning, Hinrich Schutze, ``Foundations of
   Statistical Natural Language Processing'', MIT Press, 1999.
2. Steven Bird, Ewan Klien and Edward Loper, Natural Language Processing with Python,
   O'Reilly, 2009.
3. Nitin Indurkhya, Fred J Damerau, ``Handbook of Natural Language
   Processing'', 2nd Edition, CRC Press, 2010.


Projects:
1. NLTK -- Natural Language Tool Kit - http://www.nltk.org/.
2. http://nlp-iiith.vlabs.ac.in/
3. https://www.tensorflow.org/tutorials/text/nmt_with_attention
4. Yoav Goldberg, "Neural Network Methods for Natural Language
   Processing", Synthesis Lectures on Human Language Technologies,
   Morgan & Claypool publishers, 2017.
 
